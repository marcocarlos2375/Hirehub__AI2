#!/bin/bash

# Populate Grafana with Real Metrics by calling API endpoints
# This generates actual workflow activity that gets tracked

echo "================================================================================"
echo "  POPULATING GRAFANA WITH REAL METRICS"
echo "================================================================================"

API_URL="http://localhost:8001"

echo -e "\nüìä Generating metrics by calling actual API workflows..."
echo -e "   (This will create real metrics that Grafana can display)\n"

# Function to call adaptive questions workflow
call_workflow() {
    local SKILL=$1
    local PRIORITY=$2
    local NUM=$3

    echo "   [$NUM/10] Testing skill: $SKILL ($PRIORITY priority)"

    # Start adaptive question
    curl -s -X POST "$API_URL/api/adaptive-questions/start" \
        -H "Content-Type: application/json" \
        -d "{
            \"gap_info\": {
                \"title\": \"$SKILL\",
                \"description\": \"Testing $SKILL for metrics\",
                \"priority\": \"$PRIORITY\"
            },
            \"question_id\": \"metrics-test-$NUM\",
            \"session_id\": \"grafana-test-session\"
        }" > /dev/null 2>&1

    if [ $? -eq 0 ]; then
        echo "      ‚úÖ Workflow executed"
    else
        echo "      ‚ö†Ô∏è  Workflow had issues (this is OK for testing)"
    fi

    # Small delay
    sleep 0.5
}

# Run 10 test workflows with different skills
call_workflow "Docker" "CRITICAL" 1
call_workflow "Kubernetes" "CRITICAL" 2
call_workflow "AWS Lambda" "IMPORTANT" 3
call_workflow "React" "IMPORTANT" 4
call_workflow "PostgreSQL" "MEDIUM" 5
call_workflow "Redis" "MEDIUM" 6
call_workflow "GraphQL" "NICE_TO_HAVE" 7
call_workflow "TypeScript" "NICE_TO_HAVE" 8
call_workflow "Jest" "LOW" 9
call_workflow "Webpack" "LOW" 10

echo -e "\n‚úÖ Test workflows completed!"
echo -e "\nüìä Checking metrics endpoints..."

# Check if metrics were generated
DASHBOARD_DATA=$(curl -s "$API_URL/api/metrics/dashboard")

# Try to extract some metrics
PERF_COUNT=$(echo $DASHBOARD_DATA | python3 -c "import sys, json; data=json.load(sys.stdin); print(data.get('performance', {}).get('all_operations', {}).get('count', 0))" 2>/dev/null || echo "0")

COST=$(echo $DASHBOARD_DATA | python3 -c "import sys, json; data=json.load(sys.stdin); print(data.get('costs', {}).get('total_cost_usd', 0))" 2>/dev/null || echo "0")

echo -e "\nüìà Metrics Generated:"
echo -e "   Performance operations: $PERF_COUNT"
echo -e "   Total cost: \$$COST"

if [ "$PERF_COUNT" != "0" ]; then
    echo -e "\n   ${GREEN}‚úÖ Metrics are being tracked!${NC}"
else
    echo -e "\n   ‚ö†Ô∏è  No metrics captured yet"
    echo -e "   This might be normal if workflows need instrumented nodes"
fi

echo -e "\n================================================================================"
echo -e "  NEXT STEPS"
echo -e "================================================================================"

echo -e "\n1. Open Grafana dashboard:"
echo -e "   üëâ http://localhost:3001/d/4d99bc15-6a7d-4396-9d23-ef7d7b3e92c0/hirehub-metrics-dashboard"

echo -e "\n2. Refresh the dashboard (it auto-refreshes every 10s)"

echo -e "\n3. If you still see 'No data':"
echo -e "   ‚Ä¢ The workflows might need instrumented nodes to track metrics"
echo -e "   ‚Ä¢ Check core/adaptive_question_graph.py uses instrumented imports"
echo -e "   ‚Ä¢ Metrics are in-memory and reset on API restart"

echo -e "\n4. To see metrics immediately (for testing):"
echo -e "   ‚Ä¢ The dashboard is configured correctly"
echo -e "   ‚Ä¢ The issue is that metrics need to be generated by ACTUAL workflow runs"
echo -e "   ‚Ä¢ Or instrumented nodes need to be integrated"

echo -e "\n================================================================================"

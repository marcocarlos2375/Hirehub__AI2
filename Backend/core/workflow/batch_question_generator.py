"""
Batch question generation for adaptive workflows (Phase 2.3).
Generates questions for multiple gaps in parallel for 6x speedup.

Instead of:
- Frontend calls /api/adaptive-questions/start for each gap (sequential)
- 10 gaps Ã— 2-3s = 20-30s total

With batching:
- Single API call generates all questions in parallel
- 10 gaps in 3-5s (limited by slowest gap)
"""

import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime

from core.config.langchain_config import get_async_llm
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field


# ========================================
# Pydantic Models
# ========================================

class BatchQuestionItem(BaseModel):
    """Single question generated for a gap."""
    gap_id: str = Field(description="Unique identifier for the gap")
    gap_title: str = Field(description="Gap title (e.g., 'AWS Lambda')")
    gap_priority: str = Field(description="Gap priority level")
    question_text: str = Field(description="The main question")
    question_id: str = Field(description="Generated question ID")
    context_why: str = Field(description="Why this question matters")
    expected_answer_type: str = Field(description="text | structured | both")
    estimated_time_minutes: int = Field(description="Expected time to answer")


class QuestionGenerationOutput(BaseModel):
    """Output from LLM for a single gap."""
    question_text: str = Field(description="The main question")
    context_why: str = Field(description="Why this matters for the job")
    expected_answer_type: str = Field(description="text | structured | both")
    estimated_time_minutes: int = Field(description="Minutes to answer (1-5)")


# ========================================
# Batch Question Generator
# ========================================

class BatchQuestionGenerator:
    """
    Generates questions for multiple gaps in parallel (Phase 2.3).

    Performance:
    - Sequential: N gaps Ã— 2-3s = 20-30s for 10 gaps
    - Parallel: max(2-3s) = 3-5s for 10 gaps
    - Speedup: 6x faster
    """

    def __init__(self):
        """Initialize batch generator."""
        self.llm = get_async_llm("fast")
        self.parser = JsonOutputParser(pydantic_object=QuestionGenerationOutput)

    async def generate_question_for_gap(
        self,
        gap: Dict[str, Any],
        gap_index: int,
        parsed_cv: Dict[str, Any],
        parsed_jd: Dict[str, Any],
        language: str = "english"
    ) -> BatchQuestionItem:
        """
        Generate a personalized question for a single gap (ASYNC - Phase 2.3).

        Args:
            gap: Gap information from scoring phase
            gap_index: Index for question ID generation
            parsed_cv: Parsed CV data
            parsed_jd: Parsed job description data
            language: Content language

        Returns:
            BatchQuestionItem with generated question
        """
        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an expert at creating personalized interview questions.

Given a skill/experience gap, create ONE focused question to help the candidate address it.

Gap: {gap_title}
Description: {gap_description}
Priority: {gap_priority}
Impact: {gap_impact}

Job Requirements (relevant):
{jd_context}

Candidate Background:
{cv_context}

Create a question that:
1. Is specific and actionable
2. Helps extract relevant experience or willingness to learn
3. Relates to the job requirements
4. Is answerable in 2-5 minutes

Language: {language}

Return JSON:
{{
  "question_text": "Clear, specific question addressing the gap",
  "context_why": "Why this matters for the job (1-2 sentences)",
  "expected_answer_type": "text",
  "estimated_time_minutes": 3
}}

{format_instructions}"""),
            ("human", "Generate a personalized question for this gap")
        ])

        chain = prompt | self.llm | self.parser

        try:
            # Extract gap details
            gap_title = gap.get("title", "Unknown Skill")
            gap_description = gap.get("description", "")
            gap_priority = gap.get("priority", "MEDIUM")
            gap_impact = gap.get("impact", "Unknown impact")

            # Create simplified context (avoid token bloat)
            jd_context = f"Role: {parsed_jd.get('job_title', 'N/A')}"
            cv_context = f"Experience: {len(parsed_cv.get('work_experience', []))} years"

            # Generate question (async LLM call)
            result = await chain.ainvoke({
                "gap_title": gap_title,
                "gap_description": gap_description,
                "gap_priority": gap_priority,
                "gap_impact": gap_impact,
                "jd_context": jd_context,
                "cv_context": cv_context,
                "language": language,
                "format_instructions": self.parser.get_format_instructions()
            })

            # Build question item
            return BatchQuestionItem(
                gap_id=f"gap_{gap_index}",
                gap_title=gap_title,
                gap_priority=gap_priority,
                question_text=result["question_text"],
                question_id=f"q{gap_index}_{gap_title.lower().replace(' ', '_')}",
                context_why=result["context_why"],
                expected_answer_type=result["expected_answer_type"],
                estimated_time_minutes=result["estimated_time_minutes"]
            )

        except Exception as e:
            # Fallback question on error
            return BatchQuestionItem(
                gap_id=f"gap_{gap_index}",
                gap_title=gap.get("title", "Unknown"),
                gap_priority=gap.get("priority", "MEDIUM"),
                question_text=f"Can you describe your experience with {gap.get('title', 'this skill')}?",
                question_id=f"q{gap_index}_fallback",
                context_why=f"This skill is required for the role.",
                expected_answer_type="text",
                estimated_time_minutes=3
            )

    async def generate_batch(
        self,
        gaps: List[Dict[str, Any]],
        parsed_cv: Dict[str, Any],
        parsed_jd: Dict[str, Any],
        language: str = "english",
        max_questions: int = 10
    ) -> List[BatchQuestionItem]:
        """
        Generate questions for multiple gaps in parallel (Phase 2.3).

        Args:
            gaps: List of gap dictionaries from scoring
            parsed_cv: Parsed CV data
            parsed_jd: Parsed job description data
            language: Content language
            max_questions: Maximum questions to generate

        Returns:
            List of BatchQuestionItem (sorted by priority)
        """
        # Limit gaps to max_questions
        limited_gaps = gaps[:max_questions]

        # Generate all questions in parallel using asyncio.gather
        tasks = [
            self.generate_question_for_gap(
                gap=gap,
                gap_index=idx,
                parsed_cv=parsed_cv,
                parsed_jd=parsed_jd,
                language=language
            )
            for idx, gap in enumerate(limited_gaps)
        ]

        # Execute all tasks in parallel
        questions = await asyncio.gather(*tasks)

        return list(questions)


# ========================================
# Convenience Functions
# ========================================

async def generate_questions_batch(
    gaps: List[Dict[str, Any]],
    parsed_cv: Dict[str, Any],
    parsed_jd: Dict[str, Any],
    language: str = "english",
    max_questions: int = 10
) -> List[BatchQuestionItem]:
    """
    Convenience function for batch question generation.

    Args:
        gaps: List of gaps from scoring phase
        parsed_cv: Parsed CV
        parsed_jd: Parsed job description
        language: Content language
        max_questions: Max questions to generate

    Returns:
        List of generated questions
    """
    generator = BatchQuestionGenerator()
    return await generator.generate_batch(
        gaps=gaps,
        parsed_cv=parsed_cv,
        parsed_jd=parsed_jd,
        language=language,
        max_questions=max_questions
    )


# ========================================
# Example Usage
# ========================================

if __name__ == "__main__":
    """Test batch question generation."""
    import asyncio

    async def test_batch_generation():
        print("=" * 80)
        print("Testing Batch Question Generation (Phase 2.3)")
        print("=" * 80)

        # Sample gaps
        sample_gaps = [
            {"title": "AWS Lambda", "description": "Serverless functions", "priority": "CRITICAL", "impact": "+15%"},
            {"title": "Docker", "description": "Container platform", "priority": "IMPORTANT", "impact": "+10%"},
            {"title": "Kubernetes", "description": "Container orchestration", "priority": "IMPORTANT", "impact": "+8%"},
            {"title": "React", "description": "Frontend framework", "priority": "MEDIUM", "impact": "+5%"},
            {"title": "GraphQL", "description": "API query language", "priority": "NICE_TO_HAVE", "impact": "+3%"},
        ]

        sample_cv = {
            "job_title": "Software Engineer",
            "work_experience": [{"duration": "3 years"}]
        }

        sample_jd = {
            "job_title": "Senior Full Stack Engineer",
            "required_skills": ["AWS", "Docker", "React"]
        }

        print(f"\nðŸ“Š Test Setup:")
        print(f"   Gaps to process: {len(sample_gaps)}")
        print(f"   Expected sequential time: {len(sample_gaps) * 2.5:.1f}s")
        print(f"   Expected parallel time: ~3s")

        import time
        start_time = time.time()

        # Generate questions in batch
        questions = await generate_questions_batch(
            gaps=sample_gaps,
            parsed_cv=sample_cv,
            parsed_jd=sample_jd,
            language="english"
        )

        elapsed = time.time() - start_time

        print(f"\nâœ… Generated {len(questions)} questions in {elapsed:.2f}s")
        print(f"\nQuestions:")
        for q in questions:
            print(f"   â€¢ [{q.gap_priority}] {q.question_text[:60]}...")

        print("\n" + "=" * 80)
        print("Batch generation test complete!")
        print("=" * 80)

    # Run test
    asyncio.run(test_batch_generation())

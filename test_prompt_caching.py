"""
Test Gemini Explicit Prompt Caching

This script tests the newly implemented explicit prompt caching feature
which provides 90% discount on cached tokens (vs 75% implicit caching).

Tests cover:
1. Gap analysis endpoint
2. Question generation endpoint
3. Resume rewriting endpoint
4. Cache statistics monitoring
"""

import requests
import time
import json

API_BASE = 'http://localhost:8001'

# Sample data
jd_text = """Senior Backend Engineer - Python & Cloud Infrastructure
Requirements:
- 5+ years Python development experience
- Strong experience with FastAPI, Django, or Flask
- PostgreSQL and Redis expertise
- AWS cloud infrastructure (EC2, S3, Lambda, RDS)
- Microservices architecture experience
- CI/CD pipeline management
- Docker and Kubernetes

Responsibilities:
- Design and implement scalable RESTful APIs
- Build cloud-native microservices
- Optimize database performance
- Implement caching strategies
- Lead technical architecture decisions"""

cv_text = """John Doe - Senior Software Engineer
8 years of professional software development experience

Skills:
- Languages: Python, JavaScript, TypeScript, Go
- Frameworks: FastAPI, Django, React, Node.js
- Databases: PostgreSQL, MongoDB, Redis
- Cloud: AWS (EC2, S3, Lambda, RDS), Azure
- DevOps: Docker, Kubernetes, GitHub Actions, Jenkins
- Architecture: Microservices, Event-driven systems

Experience:
Tech Corp (2020-Present) - Senior Backend Engineer
- Built high-traffic APIs serving 1M+ requests/day
- Architected microservices platform with FastAPI
- Implemented Redis caching reducing response time by 70%
- Managed AWS infrastructure with Terraform
- Led migration to Kubernetes

StartupXYZ (2017-2020) - Backend Developer
- Developed RESTful APIs with Django
- Optimized PostgreSQL queries improving performance 3x
- Built CI/CD pipelines with Jenkins
"""

print('=' * 80)
print('GEMINI EXPLICIT PROMPT CACHING TEST')
print('=' * 80)
print()

# Get initial cache stats
print('[Step 0] Getting initial cache stats...')
initial_stats = requests.get(f'{API_BASE}/cache/stats').json()
print(f'  Application cache:')
print(f'    Total requests: {initial_stats.get("total_requests", 0)}')
print(f'    Hit rate: {initial_stats.get("hit_rate", 0)}%')
if 'prompt_caching' in initial_stats:
    pc = initial_stats['prompt_caching']
    print(f'  Prompt cache:')
    print(f'    Hits: {pc.get("prompt_cache_hits", 0)}')
    print(f'    Misses: {pc.get("prompt_cache_misses", 0)}')
    print(f'    Active caches: {pc.get("active_caches", 0)}')
print()

# ===== TEST 1: Parse Documents =====
print('=' * 80)
print('TEST 1: Parsing JD and CV')
print('=' * 80)
print()

print('[1.1] Parse Job Description...')
jd_resp = requests.post(f'{API_BASE}/api/parse', json={
    'job_description': jd_text,
    'language': 'english'
})
jd_data = jd_resp.json()
print(f'  ‚úÖ JD parsed in {jd_data.get("time_seconds", 0):.3f}s')
print()

print('[1.2] Parse CV...')
cv_resp = requests.post(f'{API_BASE}/api/parse-cv', json={
    'resume_text': cv_text,
    'language': 'english'
})
cv_data = cv_resp.json()
print(f'  ‚úÖ CV parsed in {cv_data.get("time_seconds", 0):.3f}s')
print()

# ===== TEST 2: Gap Analysis with Prompt Caching =====
print('=' * 80)
print('TEST 2: Gap Analysis (Explicit Prompt Caching)')
print('=' * 80)
print()

print('[2.1] First gap analysis (creates prompt cache)...')
start = time.time()
score_resp1 = requests.post(f'{API_BASE}/api/calculate-score', json={
    'parsed_cv': cv_data['data'],
    'parsed_jd': jd_data['data'],
    'language': 'english'
})
time1 = time.time() - start
score1 = score_resp1.json()

print(f'  Time: {time1:.3f}s')
print(f'  Score: {score1.get("overall_score", 0)}/100')
print(f'  Gaps found: {len(score1.get("gaps", {}).get("critical", []))} critical, {len(score1.get("gaps", {}).get("important", []))} important')
print()

# Check cache stats after first request
stats1 = requests.get(f'{API_BASE}/cache/stats').json()
if 'prompt_caching' in stats1:
    pc1 = stats1['prompt_caching']
    print(f'  Prompt cache after first request:')
    print(f'    Misses: {pc1.get("prompt_cache_misses", 0)} (created new cache)')
    print(f'    Active caches: {pc1.get("active_caches", 0)}')
print()

print('[2.2] Second gap analysis (SAME data - should hit prompt cache)...')
start = time.time()
score_resp2 = requests.post(f'{API_BASE}/api/calculate-score', json={
    'parsed_cv': cv_data['data'],
    'parsed_jd': jd_data['data'],
    'language': 'english'
})
time2 = time.time() - start
score2 = score_resp2.json()

print(f'  Time: {time2:.3f}s')
print(f'  Score: {score2.get("overall_score", 0)}/100')
print(f'  Speedup: {time1/time2:.1f}x faster!')
print()

# Check cache stats after second request
stats2 = requests.get(f'{API_BASE}/cache/stats').json()
if 'prompt_caching' in stats2:
    pc2 = stats2['prompt_caching']
    print(f'  Prompt cache after second request:')
    print(f'    Hits: {pc2.get("prompt_cache_hits", 0)} (reused cache!)')
    print(f'    Hit rate: {pc2.get("prompt_cache_hit_rate", 0):.1f}%')
    print(f'    Estimated savings: ${pc2.get("estimated_savings_usd", 0):.6f}')
print()

# ===== TEST 3: Question Generation with Prompt Caching =====
print('=' * 80)
print('TEST 3: Question Generation (Explicit Prompt Caching)')
print('=' * 80)
print()

print('[3.1] First question generation (creates prompt cache)...')
start = time.time()
questions_resp1 = requests.post(f'{API_BASE}/api/generate-questions', json={
    'parsed_cv': cv_data['data'],
    'parsed_jd': jd_data['data'],
    'score_result': score1,
    'language': 'english'
})
time3 = time.time() - start
questions1 = questions_resp1.json()

print(f'  Time: {time3:.3f}s')
print(f'  Questions generated: {questions1.get("total_questions", 0)}')
print(f'  Critical: {questions1.get("critical_count", 0)}, High: {questions1.get("high_count", 0)}, Medium: {questions1.get("medium_count", 0)}')
print()

print('[3.2] Second question generation (SAME data - should hit prompt cache)...')
start = time.time()
questions_resp2 = requests.post(f'{API_BASE}/api/generate-questions', json={
    'parsed_cv': cv_data['data'],
    'parsed_jd': jd_data['data'],
    'score_result': score1,
    'language': 'english'
})
time4 = time.time() - start
questions2 = questions_resp2.json()

print(f'  Time: {time4:.3f}s')
print(f'  Questions generated: {questions2.get("total_questions", 0)}')
print(f'  Speedup: {time3/time4:.1f}x faster!')
print()

# ===== FINAL STATS =====
print('=' * 80)
print('FINAL CACHE STATISTICS')
print('=' * 80)

final_stats = requests.get(f'{API_BASE}/cache/stats').json()

print()
print('Application-Level Cache (Redis + In-Memory):')
print(f'  Total requests: {final_stats.get("total_requests", 0)}')
print(f'  L1 hits (in-memory): {final_stats.get("l1_hits", 0)}')
print(f'  L2 hits (Redis): {final_stats.get("l2_hits", 0)}')
print(f'  Misses: {final_stats.get("misses", 0)}')
print(f'  Hit rate: {final_stats.get("hit_rate", 0):.1f}%')
print()

if 'prompt_caching' in final_stats:
    pc_final = final_stats['prompt_caching']
    print('Gemini Explicit Prompt Caching:')
    print(f'  Prompt cache hits: {pc_final.get("prompt_cache_hits", 0)}')
    print(f'  Prompt cache misses: {pc_final.get("prompt_cache_misses", 0)}')
    print(f'  Prompt cache errors: {pc_final.get("prompt_cache_errors", 0)}')
    print(f'  Hit rate: {pc_final.get("prompt_cache_hit_rate", 0):.1f}%')
    print(f'  Total cached tokens: {pc_final.get("total_cached_tokens", 0):,}')
    print(f'  Estimated savings: ${pc_final.get("estimated_savings_usd", 0):.6f}')
    print(f'  Active caches: {pc_final.get("active_caches", 0)}')
print()

# ===== SUMMARY =====
print('=' * 80)
print('TEST SUMMARY')
print('=' * 80)
print()

print('Performance Improvements:')
print(f'  Gap Analysis:        {time1:.3f}s ‚Üí {time2:.3f}s ({time1/time2:.1f}x speedup)')
print(f'  Question Generation: {time3:.3f}s ‚Üí {time4:.3f}s ({time3/time4:.1f}x speedup)')
print()

# Determine success
gap_analysis_cached = time2 < time1 * 0.9  # Should be faster
questions_cached = time4 < time3 * 0.9  # Should be faster

if 'prompt_caching' in final_stats:
    pc = final_stats['prompt_caching']
    prompt_cache_working = pc.get('prompt_cache_hits', 0) > 0

    print('Results:')
    print(f'  ‚úÖ Gap Analysis Caching: {"WORKING" if gap_analysis_cached else "NOT DETECTED"}')
    print(f'  ‚úÖ Question Gen Caching: {"WORKING" if questions_cached else "NOT DETECTED"}')
    print(f'  ‚úÖ Prompt Cache Active:  {"YES" if prompt_cache_working else "NO"}')
    print(f'  ‚úÖ Cost Savings:         ${pc.get("estimated_savings_usd", 0):.6f} saved')
    print()

    if prompt_cache_working and gap_analysis_cached:
        print('üéâ EXPLICIT PROMPT CACHING IS WORKING!')
        print('   You are now getting 90% discount on cached tokens (vs 75% implicit)')
    else:
        print('‚ö†Ô∏è  Prompt caching may not be working as expected')
        print('   This is normal if prompts are too different or cache expired')
else:
    print('‚ö†Ô∏è  Prompt caching stats not available')

print()
